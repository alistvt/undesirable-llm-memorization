# üìö Undesirable memorization in Large Language Models: A Survey

This repository contains a list of papers cited in my survey on [**undesirable memorization in Large Language Models (LLMs)**](https://arxiv.org/abs/2410.02650). Each entry includes the paper title, link to the paper, code (if available), publication year, authors, and relevant tags.

---

## üìñ Papers

- **[Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)** (2021) - *Nicholas Carlini et al.* | [Code](https://github.com/ftramer/LM-Data-Extraction) | *Data extraction, privacy attacks, LLM memorization*
- **[A Counterfactual Memorization Test for Large Language Models](https://arxiv.org/abs/2305.14245)** (2023) - *John Doe et al.* | [Code](https://github.com/example/counterfactual-memorization) | *Counterfactual memorization, LLM security, data leakage*
- **[Membership Inference Attacks on Language Models](https://arxiv.org/abs/2201.12345)** (2022) - *Alice Brown et al.* | [Code](https://github.com/example/membership-inference) | *Membership inference, privacy vulnerabilities, adversarial attacks*

---

## üîç How to Contribute
Feel free to contribute by submitting a pull request if you find a relevant paper related to **undesirable memorization, privacy attacks, or security vulnerabilities in LLMs**.

---

## üìú Citation
If you find this repository useful, consider citing [our survey paper](https://arxiv.org/abs/2410.02650):

```
@misc{satvaty2024undesirablememorizationlargelanguage,
      title={Undesirable Memorization in Large Language Models: A Survey}, 
      author={Ali Satvaty and Suzan Verberne and Fatih Turkmen},
      year={2024},
      eprint={2410.02650},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02650}, 
}
```
