# üìö Undesirable memorization in Large Language Models: A Survey

This repository contains a list of papers cited in our survey on [**undesirable memorization in Large Language Models (LLMs)**](https://arxiv.org/abs/2410.02650). Each entry includes the paper title, link to the paper, code (if available), publication year, authors, and relevant tags.

---

## üìñ Papers


- **[Quantifying memorization across neural language models](https://arxiv.org/abs/2202.07646)** (2022) - *Nicholas Carlini et al.*

- **[Emergent and predictable memorization in large language models](https://arxiv.org/abs/2304.11158)** (2024) - *Stella Biderman et al.*

- **[Counterfactual memorization in neural language models](https://arxiv.org/abs/2112.12938)** (2023) - *Chiyuan Zhang et al.*

- **[Generalization through memorization: Nearest neighbor language models](https://openreview.net/forum?id=HklBjCEKvH)** (2019) - *Urvashi Khandelwal et al.*

- **[Preventing verbatim memorization in language models gives a false sense of privacy](https://arxiv.org/abs/2210.17546)** (2022) - *Daphne Ippolito et al.*

- **[Mitigating Memorization In Language Models](https://arxiv.org/abs/2410.02159)** (2024) - *Mansi Sakarvadia et al.*

- **[PreCog: Exploring the relation between memorization and performance in pre-trained language models](https://arxiv.org/abs/2305.04673)** (2023) - *Leonardo Ranaldi et al.*

- **[How bpe affects memorization in transformers](https://arxiv.org/abs/2110.02782)** (2021) - *Eugene Kharitonov et al.*

- **[Does learning require memorization? a short tale about a long tail](https://arxiv.org/abs/1906.05271)** (2020) - *Vitaly Feldman*

- **[Memorization in nlp fine-tuning methods]()** (2022) - *Fatemehsadat Mireshghallah et al.*

- **[Are language models leaking personal information?](https://arxiv.org/abs/2205.12506)** (2023) - *Jie Huang et al.*

- **[The secret sharer: Evaluating and testing unintended memorization in neural networks](https://arxiv.org/abs/1802.08232)** (2019) - *Nicholas Carlini et al.*

- **[Memorization without overfitting: Analyzing the training dynamics of large language models](https://arxiv.org/abs/2205.10770)** (2022) - *Kushal Tirumala et al.*

- **[Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models](https://arxiv.org/abs/2404.06209)** (2024) - *Sebastian Bordt et al.*

- **[Scaling laws for fact memorization of large language models](https://arxiv.org/abs/2406.15720)** (2024) - *Xingyu Lu et al.*

- **[LLMs and Memorization: On Quality and Specificity of Copyright Compliance](https://arxiv.org/abs/2405.18492)** (2024) - *Felix B Mueller et al.*

- **[Exploring Memorization and Copyright Violation in Frontier LLMs: A Study of the New York Times v. OpenAI 2023 Lawsuit](https://arxiv.org/abs/2412.06370)** (2024) - *Joshua Freeman et al.*

- **[Preserving privacy through dememorization: An unlearning technique for mitigating memorization risks in language models](https://aclanthology.org/2023.emnlp-main.265/)** (2023) - *Aly Kassem et al.*

- **[Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs](https://arxiv.org/abs/2406.10209)** (2024) - *Abhimanyu Hans et al.*

- **[The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text Memorization](https://aclanthology.org/2024.inlg-main.30/)** (2024) - *Luka Borec et al.*

- **[Provable memorization capacity of transformers](https://openreview.net/forum?id=8JCg5xJCTPR)** (2023) - *Junghwan Kim et al.*

- **[Memorization capacity of multi-head attention in transformers](https://arxiv.org/abs/2306.02010)** (2023) - *Sadegh Mahdavi et al.*

- **[Near-duplicate sequence search at scale for large language model memorization evaluation](https://dl.acm.org/doi/10.1145/3589324)** (2023) - *Zhencan Peng et al.*

- **[Quantifying and Analyzing Entity-Level Memorization in Large Language Models](https://arxiv.org/abs/2308.15727)** (2024) - *Zhenhong Zhou et al.*

- **[Unlocking memorization in large language models with dynamic soft prompting](https://arxiv.org/abs/2409.13853)** (2024) - *Zhepeng Wang et al.*

- **[Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs](https://arxiv.org/abs/2403.04801)** (2024) - *Aly M. Kassem et al.*

- **[Learning, Forgetting, Remembering: Insights From Tracking LLM Memorization During Training](https://aclanthology.org/2024.blackboxnlp-1.4/)** (2024) - *Danny D. Leybzon et al.*

- **[Exploring memorization in fine-tuned language models](https://arxiv.org/abs/2310.06714)** (2023) - *Shenglai Zeng et al.*

- **[Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models](https://openreview.net/forum?id=7PZgCems9w)** (2024) - *Sunny Duan et al.*

- **[A Comprehensive Analysis of Memorization in Large Language Models](https://aclanthology.org/2024.inlg-main.45/)** (2024) - *Hirokazu Kiyomaru et al.*

- **[Localizing Paragraph Memorization in Language Models](https://arxiv.org/abs/2403.19851)** (2024) - *Niklas Stoehr et al.*

- **[A Multi-Perspective Analysis of Memorization in Large Language Models](https://arxiv.org/abs/2405.11577)** (2024) - *Bowen Chen et al.*

- **[Demystifying verbatim memorization in large language models](https://arxiv.org/abs/2407.17817)** (2024) - *Jing Huang et al.*

- **[Recite, reconstruct, recollect: Memorization in LMs as a multifaceted phenomenon](https://arxiv.org/abs/2406.17746)** (2024) - *USVSN Sai Prashanth et al.*

- **[Understanding transformer memorization recall through idioms](https://arxiv.org/abs/2210.03588)** (2022) - *Adi Haviv et al.*

- **[Generalisation First, Memorisation Second? Memorisation Localisation for Natural Language Classification Tasks](https://arxiv.org/abs/2408.04965)** (2024) - *Verna Dankers et al.*

- **[Rethinking llm memorization through the lens of adversarial compression](https://arxiv.org/abs/2404.15146)** (2024) - *Avi Schwarzschild et al.*

- **[Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries](https://arxiv.org/abs/2210.10750)** (2022) - *Yuxin Wen et al.*

- **[Extracting training data from large language models](https://arxiv.org/abs/2012.07805)** (2021) - *Nicholas Carlini et al.*

- **[Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models](https://arxiv.org/abs/2212.03860)** (2023) - *Gowthami Somepalli et al.*

- **[Scalable Extraction of Training Data from Production Language Models](https://arxiv.org/abs/2311.17035)** (2023) - *Milad Nasr et al.*

- **[MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](https://arxiv.org/abs/2309.04662)** (2023) - *Sneha Kudugunta et al.*

- **[Privacy Implications of Retrieval-Based Language Models](https://arxiv.org/abs/2305.14888)** (2023) - *Yangsibo Huang et al.*

- **[Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage](https://arxiv.org/abs/2305.12707)** (2023) - *Hanyin Shao et al.*

- **[Are Large Pre-Trained Language Models Leaking Your Personal Information?](https://aclanthology.org/2022.findings-emnlp.148/#:~:text=We%20find%20that%20PLMs%20do,extracted%20by%20attackers%20is%20low.)** (2022) - *Jie Huang et al.*

- **[Auditing Data Provenance in Text-Generation Models](https://dl.acm.org/doi/10.1145/3292500.3330885)** (2019) - *Congzheng Song et al.*

- **[Membership Inference Attack Susceptibility of Clinical Language Models](https://arxiv.org/abs/2104.08305)** (2021) - *Abhyuday Jagannatha et al.*

- **[Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks](https://aclanthology.org/2022.emnlp-main.570/)** (2022) - *Fatemehsadat Mireshghallah et al.*

- **[An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models](https://aclanthology.org/2022.emnlp-main.119/)** (2022) - *Fatemehsadat Mireshghallah et al.*

- **[Training Data Leakage Analysis in Language Models](https://arxiv.org/abs/2101.05405)** (2021) - *Huseyin A. Inan et al.*

- **[Membership Inference Attacks against Language Models via Neighbourhood Comparison](https://aclanthology.org/2023.findings-acl.719/)** (2023) - *Justus Mattern et al.*

- **[Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)** (2023) - *Wenjie Fu et al.*

- **[Bag of tricks for training data extraction from language models](https://arxiv.org/abs/2302.04460)** (2023) - *Weichen Yu et al.*

- **[Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/abs/2304.11158)** (2023) - *Stella Biderman et al.*

- **[Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy](https://aclanthology.org/2023.inlg-main.3/)** (2023) - *Daphne Ippolito et al.*

- **[Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499)** (2022) - *Katherine Lee et al.*

- **[Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://arxiv.org/abs/2202.06539)** (2022) - *Nikhil Kandpal et al.*

- **[Analyzing Leakage of Personally Identifiable Information in Language Models](https://arxiv.org/abs/2302.00539)** (2023) - *Nils Lukas et al.*

- **[ProPILE: Probing Privacy Leakage in Large Language Models](https://arxiv.org/abs/2307.01881)** (2023) - *Siwon Kim et al.*

- **[ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation](https://arxiv.org/abs/2307.04401)** (2023) - *Zhexin Zhang et al.*

- **[Large-Scale Differentially Private BERT](https://arxiv.org/abs/2108.01624)** (2022) - *Rohan Anil et al.*

- **[Learning and Evaluating a Differentially Private Pre-trained Language Model](https://aclanthology.org/2021.findings-emnlp.102/)** (2021) - *Shlomo Hoory et al.*

- **[Selective Differential Privacy for Language Modeling](https://arxiv.org/abs/2108.12944)** (2022) - *Weiyan Shi et al.*

- **[Provably Confidential Language Modelling](https://arxiv.org/abs/2205.01863)** (2022) - *Xuandong Zhao et al.*

- **[Differentially Private Language Models Benefit from Public Pre-training](https://aclanthology.org/2020.privatenlp-1.5/)** (2020) - *Gavin Kerrigan et al.*

- **[Large Language Models Can Be Strong Differentially Private Learners](https://arxiv.org/abs/2110.05679)** (2022) - *Xuechen Li et al.*

- **[An Efficient DP-SGD Mechanism for Large Scale NLP Models](https://arxiv.org/abs/2107.14586)** (2022) - *Christophe Dupuy et al.*

- **[Knowledge Unlearning for Mitigating Privacy Risks in Language Models](https://arxiv.org/abs/2210.01504)** (2023) - *Joel Jang et al.*

- **[Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238)** (2023) - *Ronen Eldan et al.*

- **[Detecting Pretraining Data from Large Language Models](https://arxiv.org/abs/2310.16789)** (2024) - *Weijia Shi et al.*

- **[Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection](https://arxiv.org/abs/2309.03057)** (2023) - *Yu Chen et al.*

- **[CipherGPT: Secure Two-Party GPT Inference](https://eprint.iacr.org/2023/1147)** (2023) - *Xiaoyang Hou et al.*

- **[Are clinical BERT models privacy preserving? The difficulty of extracting patient-condition associations](https://ceur-ws.org/Vol-3068/short1.pdf)** (2021) - *Thomas Vakili et al.*

- **[Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?](https://aclanthology.org/2021.naacl-main.73/)** (2021) - *Eric Lehman et al.*

- **[The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2402.16893)** (2024) - *Shenglai Zeng et al.*

- **[Measuring Forgetting of Memorized Training Examples](https://arxiv.org/abs/2207.00099)** (2023) - *Matthew Jagielski et al.*

- **[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)** (2023) - *Jiaao Chen et al.*

- **[In-Context Unlearning: Language Models as Few Shot Unlearners](https://arxiv.org/abs/2310.07579)** (2024) - *Martin Pawelczyk et al.*

- **[Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges](https://arxiv.org/abs/2311.15766)** (2023) - *Nianwen Si et al.*

- **[Large Language Model Unlearning](https://arxiv.org/abs/2310.10683)** (2024) - *Yuanshun Yao et al.*

- **[Privacy in the Time of Language Models](https://dl.acm.org/doi/10.1145/3539597.3575792)** (2023) - *Charith Peris et al.*

- **[Detecting Unintended Memorization in Language-Model-Fused ASR](https://arxiv.org/abs/2204.09606)** (2022) - *W. Ronny Huang et al.*

- **[Assessing Unintended Memorization in Neural Discriminative Sequence Models](https://link.springer.com/chapter/10.1007/978-3-030-58323-1_29)** (2020) - *Mossad Helali et al.*

- **[Improved Membership Inference Attacks Against Language Classification Models](https://arxiv.org/abs/2310.07219)** (2024) - *Shlomit Shachor et al.*

- **[Membership Inference Attacks Against NLP Classification Models](https://openreview.net/forum?id=74lwg5oxheC)** (2021) - *Virat Shejwalkar et al.*

- **[Analyzing and Defending against Membership Inference Attacks in Natural Language Processing Classification](https://ieeexplore.ieee.org/document/10020711)** (2022) - *Yijue Wang et al.*

- **[Canary Extraction in Natural Language Understanding Models](https://arxiv.org/abs/2203.13920)** (2022) - *Rahil Parikh et al.*

- **[Exploring Memorization in Fine-tuned Language Models](https://arxiv.org/abs/2310.06714)** (2024) - *Shenglai Zeng et al.*

- **[How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN](https://aclanthology.org/2023.tacl-1.38/)** (2023) - *R. Thomas McCoy et al.*

- **["According to ...": Prompting Language Models Improves Quoting from Pre-Training Data](https://arxiv.org/abs/2305.13252)** (2024) - *Orion Weller et al.*

- **[Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning](https://aclanthology.org/2023.acl-short.129/)** (2023) - *Mustafa Ozdayi et al.*

- **[Impact of Co-occurrence on Factual Knowledge of Large Language Models](https://aclanthology.org/2023.findings-emnlp.518/)** (2023) - *Cheongwoong Kang et al.*

- **[A Review on Language Models as Knowledge Bases](https://arxiv.org/abs/2204.06031)** (2022) - *Badr AlKhamissi et al.*

- **[Do Language Models Plagiarize?](https://arxiv.org/abs/2203.07618)** (2023) - *Jooyoung Lee et al.*

- **[How Can We Know What Language Models Know?](https://arxiv.org/abs/1911.12543)** (2020) - *Zhengbao Jiang et al.*

- **[Just Fine-tune Twice: Selective Differential Privacy for Large Language Models](https://arxiv.org/abs/2204.07667)** (2022) - *Weiyan Shi et al.*

- **[Mitigating Unintended Memorization in Language Models Via Alternating Teaching](https://arxiv.org/abs/2210.06772)** (2023) - *Zhe Liu et al.*

- **[Your Code Secret Belongs to Me: Neural Code Completion Tools Can Memorize Hard-Coded Credentials](https://dl.acm.org/doi/10.1145/3660818)** (2023) - *Yizhan Huang et al.*

- **[Planting and Mitigating Memorized Content in Predictive-Text Language Models](https://arxiv.org/abs/2212.08619)** (2022) - *C.M. Downey et al.*

- **[Memorization vs. Generalization: Quantifying Data Leakage in NLP Performance Evaluation](https://aclanthology.org/2021.eacl-main.113/)** (2021) - *Aparna Elangovan et al.*

- **[Training Production Language Models without Memorizing User Data](https://arxiv.org/abs/2009.10031)** (2020) - *Swaroop Ramaswamy et al.*
---

## üîç How to Contribute
Feel free to contribute by submitting a pull request if you find a relevant paper related to **undesirable memorization, privacy attacks, or security vulnerabilities in LLMs**.

---

## üìú Citation
If you find this repository useful, consider citing [our survey paper](https://arxiv.org/abs/2410.02650):

```
@misc{satvaty2024undesirablememorizationlargelanguage,
      title={Undesirable Memorization in Large Language Models: A Survey}, 
      author={Ali Satvaty and Suzan Verberne and Fatih Turkmen},
      year={2024},
      eprint={2410.02650},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.02650}, 
}
```
